{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a50df2b-6d7d-4f02-9a7a-ff4dd7d63758",
   "metadata": {},
   "source": [
    "## Step 0: Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d4f2f8-6bb5-4175-a6cb-f98efbfccd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/home/extra/micheal/pixel2style2pixel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9954c2-075d-41ed-a45d-f27975649409",
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "import time\n",
    "import sys\n",
    "import pprint\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "sys.path.append(\".\")\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from utils.common import tensor2im, log_input_image\n",
    "from models.psp import pSp\n",
    "\n",
    "# added imports\n",
    "import os\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "from configs.transforms_config import SegToImageTransforms\n",
    "from glob import glob\n",
    "from training.coach import Coach\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16070940-b82a-457e-87f5-2db2237d3233",
   "metadata": {},
   "source": [
    "## Step 2: Load Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad89b829-8a07-4f17-9166-41dc73810ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '/home/extra/micheal/pixel2style2pixel/experiments/ioct_seg2bscan2/checkpoints/best_model.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49d99ce-d9bf-47ee-99d5-26c064894010",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = torch.load(model_path, map_location='cpu')\n",
    "opts = ckpt['opts']\n",
    "optss = Namespace(**opts)\n",
    "optss.batch_size = 1\n",
    "optss.stylegan_weights = model_path\n",
    "optss.load_partial_weights = True\n",
    "\n",
    "coach = Coach(optss)\n",
    "\n",
    "device = torch.device(coach.opts.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de59fdf0-6df2-41bc-99bd-daa10a066e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "coach.net = coach.net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ded7f3-1e6b-4842-ae71-adb5d2d08373",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = iter(coach.train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5540b493-e838-4a7a-b886-86d0eac18a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(train_loader)\n",
    "test_label = batch[0].cuda().float()\n",
    "print('label shape', test_label.shape)\n",
    "with torch.no_grad():\n",
    "    test_bscan = coach.net(test_label)\n",
    "    print('test_bscan shape', test_bscan.shape)\n",
    "\n",
    "    \n",
    "pred = test_bscan[0][0].cpu().detach()\n",
    "bscan = batch[1][0][0]\n",
    "label = np.argmax(batch[0][0], axis=0)\n",
    "    \n",
    "fig, axes = plt.subplots(1,3, figsize=(15,5))\n",
    "# axes[0].axis('off')\n",
    "# axes[1].axis('off')\n",
    "# axes[2].axis('off')\n",
    "axes[0].imshow(pred)\n",
    "axes[0].set_xlabel('pred')\n",
    "axes[1].imshow(bscan)\n",
    "axes[1].set_xlabel('bscan')\n",
    "axes[2].imshow(label)\n",
    "axes[2].set_xlabel('label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d367f9-4e95-4691-a4af-7f747d6f712c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    test_bscan, latent = coach.net(test_label, return_latents=True)\n",
    "    bscan_a, latent_a = coach.net(latent, input_code=True, return_latents=True)\n",
    "torch.all(latent==latent_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32040f80-1940-441f-b5e2-466e2eee7662",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Debug: Compare the difference of manual loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868aa822-634f-4da3-9c6a-dc5dbe1aa340",
   "metadata": {},
   "source": [
    "Manually load image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae6cc44-e896-41c6-a07c-2a75ad6dd072",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_paths = 'data/ioct/labels/train/*'\n",
    "\n",
    "label_path = glob(label_paths)[673]\n",
    "bscan_path = label_path.split('labels')[0] + 'bscans' + label_path.split('labels')[1]\n",
    "label = imageio.imread(label_path)\n",
    "bscan = imageio.imread(bscan_path)\n",
    "label = label\n",
    "aggragated = np.concatenate((label*50, bscan), axis=1)\n",
    "plt.imshow(aggragated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f9e664-0430-4395-9e16-a08032117676",
   "metadata": {},
   "source": [
    "Load with image library:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfb392a-ae96-4d2e-860a-6acbeb514c5f",
   "metadata": {},
   "source": [
    " Load transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4faeab-b90f-4f0f-9ee0-0b43e23156bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_opt = Namespace(label_nc=5, output_nc=1)\n",
    "transform_dict = SegToImageTransforms(transform_opt).get_transforms()\n",
    "img_transforms = transform_dict['transform_inference']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c92d6be-8741-4ae5-8efc-894202a1843c",
   "metadata": {},
   "source": [
    "Convert use python codes in files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a223b821-a786-4301-9a10-dd539f0c402f",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = Image.open(label_path).convert('L')\n",
    "transformed_label = img_transforms(label)\n",
    "print('transformed label is of shape', transformed_label.shape)\n",
    "plt.imshow(np.argmax(transformed_label, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d023ac04-0fcb-4854-a0c6-d6f592e04ba1",
   "metadata": {},
   "source": [
    "Manually feed to the net work we load before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ab5021-e31d-422c-91c7-baeafbe2675e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "batched_label = transformed_label.unsqueeze(0).cuda().float()\n",
    "print('transformed label shape', batched_label.shape)\n",
    "with torch.no_grad():\n",
    "    pred_bscan = coach.net(batched_label)\n",
    "    print('test_bscan shape', pred_bscan.shape)\n",
    "\n",
    "pred_bscan = pred_bscan[0][0].cpu().detach()\n",
    "    \n",
    "fig, axes = plt.subplots(1,3, figsize=(15,5))\n",
    "axes[0].axis('off')\n",
    "axes[1].axis('off')\n",
    "axes[2].axis('off')\n",
    "axes[0].imshow(pred_bscan)\n",
    "axes[0].set_xlabel('pred')\n",
    "axes[1].imshow(cv2.resize(bscan, dsize=(256, 256)))\n",
    "axes[1].set_xlabel('bscan')\n",
    "axes[2].imshow(label.resize((256, 256)))\n",
    "axes[2].set_xlabel('label')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458c7040-31b7-44db-bb76-1e27ec8b2d4a",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Custom functions fot transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc227e8e-3ac3-4c14-9a82-cd9278b3c2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "class Conver2Uint8(torch.nn.Module):\n",
    "    '''\n",
    "    Resize input when the target dim is not divisible by the input dim\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, img):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img (PIL Image or Tensor): Image to be scaled.\n",
    "\n",
    "        Returns:\n",
    "            PIL Image or Tensor: Rescaled image.\n",
    "        \"\"\"\n",
    "        img = torch.round(img * 255)\n",
    "        return img\n",
    "    \n",
    "class MyResize(torch.nn.Module):\n",
    "    '''\n",
    "    Resize input when the target dim is not divisible by the input dim\n",
    "    '''\n",
    "    def __init__(self, size):\n",
    "        super().__init__()\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self, img):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img (PIL Image or Tensor): Image to be scaled.\n",
    "\n",
    "        Returns:\n",
    "            PIL Image or Tensor: Rescaled image.\n",
    "        \"\"\"\n",
    "        h, w = img.shape[-2], img.shape[-1]\n",
    "        target_h, target_w = self.size\n",
    "        assert h % target_h == 0, f\"target_h({target_h}) must be divisible by h({h})\"\n",
    "        assert w % target_w == 0, f\"target_w({target_w}) must be divisible by w({w})\"\n",
    "        # Resize by assigning the max value of each pixel grid\n",
    "        kernel_h = h // target_h\n",
    "        kernel_w = w // target_w\n",
    "        img_target = torch.nn.functional.max_pool2d(img, kernel_size=(kernel_h, kernel_w), stride=(kernel_h, kernel_w))\n",
    "        return img_target\n",
    "    \n",
    "class ToOneHot(torch.nn.Module):\n",
    "    '''\n",
    "    Convert input to one-hot encoding\n",
    "    '''\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def forward(self, img):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img (Tensor): Image to be scaled of shape (1, h, w).\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Rescaled image.\n",
    "        \"\"\"\n",
    "        img = img.long()[0]\n",
    "        img = torch.nn.functional.one_hot(img, num_classes=self.num_classes)\n",
    "        img = img.permute(2, 0, 1)\n",
    "        return img\n",
    "\n",
    "resize = MyResize((256, 256))\n",
    "off_resize = transforms.Resize((256, 256), interpolation=transforms.InterpolationMode.NEAREST)\n",
    "my_transforms = transforms.Compose([ transforms.ToTensor(), Conver2Uint8(), resize, ToOneHot(5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dae044c-e714-4f32-9a59-6370e9051ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pil_img = Image.open(label_path).convert('L')\n",
    "transformed_img = img_transforms(pil_img)\n",
    "reverted_label = np.argmax(transformed_img, axis=0)\n",
    "transformed_img = my_transforms(pil_img)\n",
    "# np_img = np.asarray(pil_img)\n",
    "# print(np_img.shape)\n",
    "# plt.imshow(np_img==1)\n",
    "print(np.unique(transformed_img))\n",
    "plt.imshow(np.argmax(transformed_img, axis=0) == 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97dd4f1a-161a-45a8-819c-bd4f3cc8757d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(transformed_img.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01dbad2e-7eb0-4fe4-bd88-cee07275c490",
   "metadata": {},
   "source": [
    "Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69130993-2ba9-4a88-9097-927a01bbe2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coach.net(img_transforms(label))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49619c4c-3b84-45d8-9911-dcce7e4f20c4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 3: Locate instruments in the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2f8127-ac39-4cdf-8c98-6fe0003ce92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coor_avg(label_map, label_num):\n",
    "    assert len(label_map.shape)==2, f'label map must be a 2D array, but got shape {label_map.shape}'\n",
    "    if isinstance(label_map, torch.Tensor):\n",
    "        label_map = label_map.numpy()\n",
    "    coords = np.argwhere(label_map==label_num)\n",
    "    assert coords.shape[1] == len(label_map.shape), f'coords.shape[1] must equals ndim, but got shape {coords.shape}'\n",
    "    x_avg = np.average(coords[:, 0])\n",
    "    y_avg = np.average(coords[:, 1])\n",
    "    n_label = len(coords)\n",
    "    return (x_avg, y_avg, n_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79927a1f-6c14-4988-9c80-11df5464793c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "label_paths = 'data/ioct/labels/train/*'\n",
    "\n",
    "label_path = glob(label_paths)[5843]\n",
    "bscan_path = label_path.split('labels')[0] + 'bscans' + label_path.split('labels')[1]\n",
    "label = imageio.imread(label_path) == 2\n",
    "bscan = imageio.imread(bscan_path)\n",
    "label = label\n",
    "aggragated = np.concatenate((label*50, bscan), axis=1)\n",
    "plt.imshow(aggragated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a835fe5e-9db6-40cb-88a0-fc62c29b029f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "coord_list = []\n",
    "\n",
    "for idx, (label, bscan) in enumerate(tqdm(coach.train_dataset)):\n",
    "    label = label.argmax(dim=0)\n",
    "    x4, y4, n4 = get_coor_avg(label, 4)\n",
    "    x2, y2, n2 = get_coor_avg(label, 2)\n",
    "    coord_list.append({\n",
    "        'idx': idx,\n",
    "        'l4': (x4, y4), \n",
    "        'n4': n4,\n",
    "        'l2': (x2, y2),\n",
    "        'n2': n2\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a0f297-8d86-414a-b975-cc08d2cce3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(r\"experiments/coords.pickle\", \"wb\") as output_file:\n",
    "    pickle.dump(coord_list, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ceb4b1-83ac-4f59-8cca-2e62e07a420e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"experiments/coords.pickle\", \"rb\") as f:\n",
    "    coord_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c53bb8f-2012-4b4e-b382-3f1c45594d2e",
   "metadata": {},
   "source": [
    "## Step 4: Sort according to label2's x-coordinate\n",
    "\n",
    "label 2 is the instrument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b4697c-9b54-4ad3-ae92-5100afd3a712",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def key_x2(item):\n",
    "    return item['l2'][0]\n",
    "\n",
    "l2x = [e for e in coord_list if not np.isnan(e['l2'][0]) and not np.isnan(e['l2'][1]) and e['n2'] > 100]\n",
    "print(f\"{len(l2x)} / {len(coord_list)} has instrument inside\")\n",
    "l2x.sort(key=key_x2)\n",
    "\n",
    "print('\\nStatistics of x-axis:')\n",
    "l2x_extracted = [e['l2'][0] for e in l2x]\n",
    "df_l2x = pd.DataFrame(l2x_extracted)\n",
    "df_l2x.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946ec560-2944-4498-8f90-029beed8cf56",
   "metadata": {},
   "source": [
    "## Step 5: Extract style latent of each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea41aa17-008a-4c58-819e-93934885b8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "l2x_latents = []\n",
    "\n",
    "for e in tqdm(l2x):\n",
    "    label, bscan = coach.train_dataset[e['idx']]\n",
    "    label = label.unsqueeze(0).float().to(device)\n",
    "    with torch.no_grad():\n",
    "        pred, latent = coach.net(label, return_latents=True)\n",
    "    l2x_latents.append(latent.detach().cpu().numpy()[0])\n",
    "\n",
    "print(f\"length of l2x latent: {len(l2x_latents)}\")\n",
    "print(f\"each latent is of shape: {l2x_latents[0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be127ff-a695-4472-b704-0fc4297d8f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"experiments/latents_l2x.pickle\", \"wb\") as output_file:\n",
    "    pickle.dump(l2x_latents, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1f082e-c9ea-4d29-83b8-87e2a3668532",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"experiments/latents_l2x.pickle\", \"rb\") as f:\n",
    "    l2x_latents = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5d1f79-ef75-4caa-8cc2-8be516fd5049",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 6: Find deviation of positive samples from average"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e54a81-9cfd-439e-881e-06c906d16bcf",
   "metadata": {},
   "source": [
    "### 1. Compute mean and standard deviation of all style vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bdb0bd-b094-4426-81d1-e061244a6c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "l2x_latent_flat = np.array([e.reshape(-1) for e in l2x_latents])\n",
    "l2x_latent_flat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e855cf-7438-406a-ac44-e9a39f7744b4",
   "metadata": {},
   "source": [
    "Mean & standard deviation:\n",
    "\n",
    "`p` means population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97c575a-a4dc-49ba-8bdf-8cc59aee391a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_p = np.mean(l2x_latent_flat, axis=0)\n",
    "print('mean is of shape', mean_p.shape)\n",
    "std_p = np.std(l2x_latent_flat, axis=0)\n",
    "print('standard deviation is of shape', std_p.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a2f487-0acf-4a80-88df-6ff77b0d317c",
   "metadata": {},
   "source": [
    "### 2. Find positive examples\n",
    "\n",
    "Set the right most 200 images as positive?\n",
    "\n",
    "Here `e` means exempler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a547169-5cca-4f40-8730-23c1fe0fb8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "latents_e = l2x_latent_flat[-200:, :]\n",
    "latents_e.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e52494-f76d-4d76-a7f6-a57ada9aae8d",
   "metadata": {},
   "source": [
    "### 3. Compute the normalized difference of \n",
    "\n",
    "Normailized difference of each positive sample from population distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf40af1-4a29-4447-8964-6c5265cbcbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_diff_e = (latents_e-mean_p)/std_p\n",
    "normalized_diff_e.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce386902-6dab-46c5-a569-388c61f0629b",
   "metadata": {},
   "source": [
    "### 4. Compute the mean and std of the normalized difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932533c0-238a-4e56-ae28-179a089ba7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_e = np.mean(normalized_diff_e, axis=0)\n",
    "print('mean is of shape', mean_e.shape)\n",
    "std_e = np.std(normalized_diff_e, axis=0)\n",
    "print('standard deviation is of shape', std_e.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9b3c89-3d70-4e55-be60-4238b59c9a3a",
   "metadata": {},
   "source": [
    "### 5. Compute the impact factor\n",
    "\n",
    "which is the magnitude of mean divided by the standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39a7bed-a2c3-4fed-af80-6e598f97741d",
   "metadata": {},
   "outputs": [],
   "source": [
    "impact_e = np.absolute(mean_e) / std_e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb517f94-2f82-49c1-9d6f-310641b8066e",
   "metadata": {},
   "source": [
    "Show statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bce5e5-a7b4-4372-b049-4dbe6801f68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "impact_df = pd.DataFrame(impact_e)\n",
    "impact_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6be461f-56d7-42ae-a281-bd232f03e90b",
   "metadata": {},
   "source": [
    "### 6. Sort and list the 10 most impactful latent location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2239268-19b0-4696-b199-46102ec970bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_impact = np.argsort(-impact_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f31fdb-ffed-47dd-b62b-0dffd31d5a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_with_impact = [(sorted_impact[i], impact_e[sorted_impact[i]]) for i in range(100)]\n",
    "print(idx_with_impact)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5588a5e0-a82d-4b3b-840a-459eb0c76cc5",
   "metadata": {},
   "source": [
    "## Step 7: Try to manupulate a picture?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4593276-b286-49ca-b0f7-be149a07bb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent = torch.from_numpy(l2x_latent_flat[8].reshape(18, 512)).float().to(device)\n",
    "latent.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9706c2e5-2ec5-4932-acda-55373acfde40",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output, latent_temp = coach.net.decoder([latent.unsqueeze(0)], input_is_latent=True, randomize_noise=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b95e587-e786-498c-ba2b-e004803ec468",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(output.detach().cpu()[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c274df4-d883-47d0-b8ff-f538745faa0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent = latent.reshape(-1)\n",
    "latent[2133] -= 10\n",
    "latent[1077] -= 10\n",
    "latent = latent.reshape(18, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ce6913-0d66-443e-a43e-c235ac66a4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output, latent_temp = coach.net.decoder([latent.unsqueeze(0)], input_is_latent=True, randomize_noise=True)\n",
    "plt.imshow(output.detach().cpu()[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b1588c-9582-41dc-85e0-a49c89db1df8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.7 ('psp_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "dee75520f43c5ee5e24c781e9aa79b8ff16e051ce0a2a9b8894c8d259d6c1da2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
