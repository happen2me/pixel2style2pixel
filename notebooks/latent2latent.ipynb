{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83baacb2-64a2-485d-ad71-ff8347c3df83",
   "metadata": {},
   "source": [
    "# Latent to Latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa49eda9-e586-4344-b190-8a14ca5df0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline  \n",
    "import os\n",
    "os.chdir('/home/extra/micheal/pixel2style2pixel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d331108-02b6-44f6-a47a-32bd1cae9f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "from training.coach import Coach\n",
    "from models.regressor import Regressor\n",
    "from models.latent2latent import Latent2Latent, LightningLatent2Latent\n",
    "from utils.latent_utils import train_imgs_batch\n",
    "from utils.regressor_utils import attribute_label_from_segmentation, get_train_loader_from_checkpoint\n",
    "from utils.latent_utils import modify_attribute, get_latent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b90620-d2f7-4f1b-be75-a699b41cbe2b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Compute a corelation matrix\n",
    "\n",
    "Assuming $a'$ is the attribute vector changing a single attribute. To correct unfeasible attribute combinations, we first created a correlation matrix based on all the attributes in the training dataset. Using this data we create a corrected vector $a'_c$ by multiplying the corresponding row elements from the correlation matrix with the $a'$ if the elements are larger than a threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24ddde5-9236-4fdb-a732-9bb879282425",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1.1 Acquire attributes from training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8d12bd-23df-4824-874f-141eee277331",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(attribute_label_from_segmentation)\n",
    "help(get_train_loader_from_checkpoint)\n",
    "# train_loader = get_train_loader_from_checkpoint('/home/extra/micheal/pixel2style2pixel/experiments/ioct_seg2bscan2/checkpoints/best_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf6a6d6-588b-44a6-9c73-60ed2ea2971f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_attributes = []\n",
    "for batch in tqdm(train_loader):\n",
    "    seg, bscan = batch\n",
    "    attributes = [attribute_label_from_segmentation(s) for s in seg]\n",
    "    all_attributes += attributes\n",
    "all_attributes = np.array(all_attributes)\n",
    "print(\"attributes shape\", all_attributes.shape)\n",
    "all_attributes[:, 1] /= 1024\n",
    "all_attributes[:, 2] /= 512\n",
    "all_attributes[:, 3] /= 1024\n",
    "all_attributes[:, 4] /= 512\n",
    "all_attributes[:, 6] /= 1024\n",
    "all_attributes[:, 7] /= 512\n",
    "all_attributes[:, 8] /= 1024\n",
    "all_attributes[:, 9] /= 512"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b43bdce-289b-43db-91c3-ce4c8a4080fd",
   "metadata": {},
   "source": [
    "Save for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f23f0e-6b77-4cc8-a76f-40ef3f5a1f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"artifacts/objects/attributes_train.np\", \"wb\") as f:\n",
    "    np.save(f, all_attributes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67eeead5-8398-41e5-b1a6-4e716a763554",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 1.2 Compute and plot correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74b4071-cc25-4edd-b18f-d80bd019f0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_attributes = all_attributes.transpose()\n",
    "print(\"all attributes transposed shape\", all_attributes.shape)\n",
    "R1 = np.corrcoef(all_attributes)\n",
    "print(\"correlation matrix shape\", R1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63e46e7-ea21-4c26-b532-f6af0bf97e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.matshow(R1)\n",
    "cb = plt.colorbar()\n",
    "cb.ax.tick_params(labelsize=14)\n",
    "plt.title('Correlation Matrix', fontsize=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e7ae31-0d71-4ddc-bcc5-e2a076850726",
   "metadata": {},
   "source": [
    "Save correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8275e4b6-0e4b-43b9-902c-97382364fd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"artifacts/objects/corrmat10.np\", \"wb\") as f:\n",
    "    np.save(f, R1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab278010-5df8-491e-ae3c-a5c53ee1d529",
   "metadata": {},
   "source": [
    "### 1.3 Load directly if already saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8e33a4-ec8c-491b-99b1-43fa6f20028c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "with open(\"artifacts/objects/corrmat10.np\", \"rb\") as f:\n",
    "    R1 = np.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc77fb0a-b2b2-4fae-ab5a-aef62add11f0",
   "metadata": {},
   "source": [
    "## 2. Define parameter correction method\n",
    "\n",
    "Currently I decide to only experiment on the horizontal location of the instrument."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3317f3-297f-4c21-a60a-eef2d1447410",
   "metadata": {},
   "source": [
    "### 2.0 Simply import everything\n",
    "Since they are moved to files, we simply import them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e17ccea-f263-43ad-aabf-18e338a3b09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(attribute_label_from_segmentation)\n",
    "help(get_train_loader_from_checkpoint)\n",
    "help(modify_attribute)\n",
    "help(get_latent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46aaff9-299b-45a6-bde6-e54fb3b698b6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 2.1 Define functions for modifying attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91b60ed-b00b-4e10-906d-49310039b052",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.latent_utils import modify_attribute, get_latent\n",
    "help(modify_attribute)\n",
    "help(get_latent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c201e9-a290-4d88-a5f1-2f8cae3128a0",
   "metadata": {},
   "source": [
    "## 3. Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae58a53a-df7d-42bf-a044-9f9fea84c4b6",
   "metadata": {},
   "source": [
    "### 3.1 Define functions for training a batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191ded4c-7231-4da5-bb66-aaada0189dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(Latent2Latent)\n",
    "help(train_imgs_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43384cfa-1b7b-4498-957e-53bf94bd7bfd",
   "metadata": {},
   "source": [
    "### 3.2 Prepare"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd040a1-f0a3-4191-876f-fef12f08fdd8",
   "metadata": {},
   "source": [
    "Testï¼š\n",
    "\n",
    "- Load regressor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656ae982-adda-4ddb-84dd-a66dec21e0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "regressor = Regressor()\n",
    "regressor.load_state_dict(torch.load(\"artifacts/weights/regressor.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593aa8c4-e6c5-4a7a-bece-6e550883c3a1",
   "metadata": {},
   "source": [
    "- Load stylegan model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992ae6ad-ba7b-48b6-8c2b-77963c88a4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '/home/extra/micheal/pixel2style2pixel/experiments/ioct_seg2bscan2/checkpoints/best_model.pt'\n",
    "\n",
    "ckpt = torch.load(model_path, map_location='cpu')\n",
    "opts = ckpt['opts']\n",
    "optss = Namespace(**opts)\n",
    "optss.batch_size = 16\n",
    "optss.stylegan_weights = model_path\n",
    "optss.load_partial_weights = True\n",
    "\n",
    "coach = Coach(optss)\n",
    "\n",
    "device = torch.device(coach.opts.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99613f69-61e0-4d9e-a648-0867ab0a6a6f",
   "metadata": {},
   "source": [
    "- define latent model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8dd7a2-529f-4118-948a-9723ee81f495",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent2latent = Latent2Latent().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa90f851-8695-4bd7-aaec-011512948d8f",
   "metadata": {},
   "source": [
    "Validation check for latent to latent\n",
    "```\n",
    "seg, bscan = coach.train_dataset[0]\n",
    "print(\"seg shape\", seg.shape)\n",
    "pred, latent, codes = get_latent(coach.net, seg, device)\n",
    "print(f\"pred shape: {list(pred.size())}, latent shape: {list(latent.size())}, codes shape {list(codes.shape)}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ffb6e3-72a7-46fa-adb5-8c998879d7cc",
   "metadata": {},
   "source": [
    "- Validity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc261e7a-162c-462a-a0cf-a7e2c016d9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(coach.train_dataloader))\n",
    "segs, bscans = batch\n",
    "\n",
    "device = torch.device('cuda')\n",
    "segs = segs.to(device).float()\n",
    "regressor = regressor.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    loss = train_imgs_batch(segs, coach.net, regressor, latent2latent, device, R1)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fe4ba2-b047-416d-8e54-b23f6bd82cfb",
   "metadata": {},
   "source": [
    "### 3.2 Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269b6eea-629d-48e0-b86b-53bdd1489ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "optimizer = optim.AdamW(latent2latent.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e2533f-32a0-4b84-8d2c-5b03a7c972a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in coach.net.parameters():\n",
    "    p.requires_grad = False\n",
    "for p in regressor.parameters():\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4341e755-7051-4ff6-a0d1-0cedfe074f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in tqdm(range(10)):\n",
    "    pbar = tqdm(coach.train_dataloader, position=0, leave=True)\n",
    "    for segs, bscans in pbar:\n",
    "        segs = segs.to(device).float()\n",
    "        loss = train_imgs_batch(segs, coach.net, regressor, latent2latent, device, R1)\n",
    "        pbar.set_description(\"Loss {:.5f}\".format(loss.detach().cpu().item()) )\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    torch.save(latent2latent.state_dict(), \"artifacts/latent2latent.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a872bca0-867d-4bd7-90dd-53b6efbb5804",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent2latent.load_state_dict(torch.load(\"artifacts/latent2latent.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179ccddb-bdc4-427e-9ac3-b82d27966292",
   "metadata": {},
   "source": [
    "#### Train with PytorchLightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e876000b-e2ce-4b3d-aca6-105691afea21",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_latent2latent = LightningLatent2Latent(coach.net, regressor, R1).to(device)\n",
    "pl_latent2latent.load_latent_state(\"artifacts/latent2latent.pt\")\n",
    "trainer = pl.Trainer(gpus=1, max_epochs=10, weights_save_path=\"artifacts/weights/latent2latent_pl\")\n",
    "trainer.fit(model=pl_latent2latent, train_dataloaders=coach.train_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9ed215-0170-4605-91c6-a675244538be",
   "metadata": {},
   "source": [
    "## 4. Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a265914-c890-40aa-bba9-83e1bc5c02d9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 4.1 Load a sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9f0f80-7aaf-4337-b197-68f51c9de69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "seg, bscan = coach.train_dataset[69]\n",
    "seg_im = np.argmax(seg, axis=0)\n",
    "plt.imshow(seg_im)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5137e281-8780-4e64-bc16-b9e54317ea32",
   "metadata": {},
   "source": [
    "### 4.2 Change an attribute & correct the modification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2170807f-4c64-4836-9c9a-c3884f3bd14c",
   "metadata": {},
   "source": [
    "Get style vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9f2a7b-467a-451c-a081-51703db3cbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "segs = seg.unsqueeze(0).float().cuda()\n",
    "with torch.no_grad():\n",
    "    _, w_latents, w_codes = get_latent(coach.net, segs, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4d1a1d-e942-4f1d-a411-b10a010812b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "attribute = attribute_label_from_segmentation(seg, normalize=True)\n",
    "modified_attribute, actual_change = modify_attribute(attribute, R1, change_nr=1, change=1)\n",
    "modified_attribute = torch.Tensor(modified_attribute).unsqueeze(0)\n",
    "attribute = torch.Tensor(attribute).unsqueeze(0)\n",
    "delta_attributes = modified_attribute - attribute\n",
    "print(actual_change)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3987ff51-3da9-49dd-9ee9-bd1c2292b2b2",
   "metadata": {},
   "source": [
    "### 4.3 Generate a new latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d50912c-3180-4558-ac9c-ab435e7ea286",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_attributes = delta_attributes.to(device)\n",
    "w_latents = w_latents.to(device)\n",
    "with torch.no_grad():\n",
    "    w_n =  latent2latent(w_latents, delta_attributes)\n",
    "w_n = 0.7*w_latents + 0.3*w_n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f74885-51b3-4e20-b80a-d56950160f10",
   "metadata": {},
   "source": [
    "### 4.5 Generate a new sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16c736c-0a0f-4051-af13-1bdc0de4d131",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_images = coach.net(w_latents, input_code=True).detach().cpu().numpy()\n",
    "plt.imshow(generated_images[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c219fd8f-defb-47b2-a4db-87eeb7629b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_reconstruction = coach.net(segs).detach().cpu().numpy()\n",
    "plt.imshow(original_reconstruction[0][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
