{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f2ecefc-3c92-4090-983a-5326451a16c9",
   "metadata": {},
   "source": [
    "# Examine Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4968a0-f63b-4d66-b258-436fdca04a32",
   "metadata": {},
   "source": [
    "Note:\n",
    "\n",
    "```python\n",
    "layer_labels=[1,2],\n",
    "instrument_labels=[3,4]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4119a8d-66b6-4a48-9bf8-d247fb58cbc9",
   "metadata": {},
   "source": [
    "## Step 1: Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4092e210-42cb-4ab5-b27b-3b5d3bc02ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "import time\n",
    "import sys\n",
    "import pprint\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "sys.path.append(\".\")\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from datasets import augmentations\n",
    "from utils.common import tensor2im, log_input_image\n",
    "from models.psp import pSp\n",
    "\n",
    "# added imports\n",
    "import os\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "from configs.transforms_config import SegToImageTransforms\n",
    "from glob import glob\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add7ee39-cc18-41ac-827a-d4b1ffb1d60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir('/home/extra/micheal/pixel2style2pixel')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752418bf-04cf-417c-a68b-81d55046f19c",
   "metadata": {},
   "source": [
    "## Step 2: Define Inference Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7143caf7-c627-48a8-9994-417e53e8d093",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_opt = Namespace(label_nc=5, output_nc=1)\n",
    "transform_dict = SegToImageTransforms(transform_opt).get_transforms()\n",
    "\n",
    "pretrained_weight_path = '/home/extra/micheal/pixel2style2pixel/pretrained_models/psp_celebs_seg_to_face.pt'\n",
    "checkpoint_path = '/home/extra/micheal/pixel2style2pixel/experiments/ioct_seg2bscan1/checkpoints/iteration_10000.pt'\n",
    "label_paths = '/home/extra/micheal/pixel2style2pixel/data/ioct/labels/train/*'\n",
    "test_image_path = glob(label_paths)[863]\n",
    "\n",
    "EXPERIMENT_ARGS = {\n",
    "    'model_path': checkpoint_path,\n",
    "    'image_path': test_image_path,\n",
    "    'transform': transform_dict['transform_inference']\n",
    "}\n",
    "assert os.path.getsize(EXPERIMENT_ARGS['model_path']) > 1000000, 'the image file is not complete'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97bfc345-e8b3-4f9d-b755-7528ac9c67c1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Test 1: Load Original Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924c1fb5-1e5b-4966-9304-f7f41edc779e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = EXPERIMENT_ARGS['model_path']\n",
    "ckpt = torch.load(model_path, map_location='cpu')\n",
    "opts = ckpt['opts']\n",
    "opts['checkpoint_path'] = model_path\n",
    "optss = Namespace(**opts)\n",
    "optss.input_nc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdecc97-5f3d-475b-9f27-4db585e09203",
   "metadata": {},
   "outputs": [],
   "source": [
    "optss.output_nc = 1\n",
    "optss.label_nc = 5\n",
    "optss.input_nc = 5\n",
    "# optss.checkpoint_path = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a20437-ba7c-43ef-a4fc-aad1f75e7995",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_test = pSp(optss)\n",
    "# net_test.latent_avg = torch.randn((18, 512))\n",
    "net_test.load_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c909d726-9cf1-4d61-a98d-91d02a7bcc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = torch.randn((1, 5, 256, 256))\n",
    "device = torch.device('cuda:1')\n",
    "net_test = net_test.to(device)\n",
    "net_test.latent_avg = net_test.latent_avg.to(device)\n",
    "input_tensor = input_tensor.to(device)\n",
    "with torch.no_grad():\n",
    "    result_batch, latents = net_test(input_tensor.float(), randomize_noise=False, return_latents=True)\n",
    "print('result_shape', result_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6201e3b9-3a22-4990-9692-747b30518f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "latents.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1235b3e-5097-4e3d-920a-a03fb8e57244",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Test 2: Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2b9af5-982f-4d69-93f7-235da33444d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from options.train_options import TrainOptions\n",
    "from training.coach import Coach\n",
    "import json\n",
    "\n",
    "\n",
    "# if os.path.exists(opts.exp_dir):\n",
    "#     raise Exception('Oops... {} already exists'.format(opts.exp_dir))\n",
    "# os.makedirs(opts.exp_dir)\n",
    "\n",
    "opts_dict = vars(optss)\n",
    "# pprint.pprint(opts_dict)\n",
    "# with open(os.path.join(optss.exp_dir, 'opt.json'), 'w') as f:\n",
    "#     json.dump(opts_dict, f, indent=4, sort_keys=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe61825a-4353-45df-858f-b8d236357f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "optss.load_partial_weights = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ddbe4a-21cc-4219-b550-5c811cb8e711",
   "metadata": {},
   "outputs": [],
   "source": [
    "optss.device = torch.device('cuda:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3b354b-1650-4e1b-b147-66089b4b6c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from training.coach import Coach\n",
    "coach = Coach(optss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2877c84-cf0d-40b2-8217-5e14a1f44fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "coach.net.load_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4261ac9a-b841-45c3-86fa-5f6fe1b1e243",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = iter(coach.train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aed29fd-7404-4786-a90c-bc0620a55671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "batch = next(train_loader)\n",
    "test_label = batch[0].cuda().float()\n",
    "print('label shape', test_label.shape)\n",
    "with torch.no_grad():\n",
    "    test_bscan = coach.net(test_label)\n",
    "    print('test_bscan shape', test_bscan.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1b7823-aab1-42c7-b555-b95875a1e882",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor2im(var, grayscale=False):\n",
    "    var = var.cpu().detach().transpose(0, 2).transpose(0, 1).numpy()\n",
    "    var = ((var + 1) / 2)\n",
    "    var[var < 0] = 0\n",
    "    var[var > 1] = 1\n",
    "    var = var * 255\n",
    "    if grayscale:\n",
    "        im = Image.fromarray(var.astype('uint8').squeeze(axis=2), 'L')\n",
    "    else:\n",
    "        im = Image.fromarray(var.astype('uint8'))\n",
    "    return im\n",
    "\n",
    "tensor2im(test_bscan[6], grayscale=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54392357-dff2-46c2-92c6-d928158328f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2590b2b7-6e8a-4721-acbd-43ec0df8452b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f12548-2f49-4d31-a2f3-cf70bbecdb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plt.imshow(batch[1][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d8bf0f-7993-46a0-82d3-06ce589b7b8b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Test 3: Play with latent code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526ab35a-9bd5-4640-a2ba-040848519636",
   "metadata": {},
   "source": [
    "### 1. Locate the instrument in the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a881236-f6b0-4e08-90ec-48ae076c4807",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "label_paths = 'data/ioct/labels/train/*'\n",
    "\n",
    "label_path = glob(label_paths)[673]\n",
    "bscan_path = label_path.split('labels')[0] + 'bscans' + label_path.split('labels')[1]\n",
    "label = imageio.imread(label_path)\n",
    "bscan = imageio.imread(bscan_path)\n",
    "label = label\n",
    "aggragated = np.concatenate((label*50, bscan), axis=1)\n",
    "plt.imshow(aggragated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e579ac-1a64-42c5-970c-53e3ac356a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coor_avg(label_map, label_num):\n",
    "    coords = np.argwhere(label_map==label_num)\n",
    "    x_avg = np.average(coords[:, 0])\n",
    "    y_avg = np.average(coords[:, 1])\n",
    "    n_label = len(coords)\n",
    "    return (x_avg, y_avg, n_label)\n",
    "\n",
    "# label[int(x_avg)-5:int(x_avg)+5, int(y_avg)-5:int(y_avg)+5] = 5\n",
    "# plt.imshow(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4128d6-b869-4eaa-ad1b-6843d9282653",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "coord_list = []\n",
    "for label_path in tqdm(glob(label_paths)):\n",
    "    label = imageio.imread(label_path)\n",
    "    if len(coord_list) == 0:\n",
    "        print(label.shape)\n",
    "    x4, y4, n4 = get_coor_avg(label, 4)\n",
    "    x2, y2, n2 = get_coor_avg(label, 2)\n",
    "    coord_list.append({\n",
    "        'l4': (x4, y4), \n",
    "        'n4': n4,\n",
    "        'l2': (x2, y2),\n",
    "        'n2': n2,\n",
    "        'path': label_path\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b280b26-0991-4e1c-8148-15b99ceba797",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(coord_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8068fb-3d7e-414c-af11-d99c6f91901a",
   "metadata": {},
   "source": [
    "### 2. Sort based on l4.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5328ac5-3627-45d2-9d80-13fc9c09536d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "def key_x4(item):\n",
    "    return item['l4'][0]\n",
    "\n",
    "l4x = [e for e in coord_list if not np.isnan(e['l4'][0]) and not np.isnan(e['l4'][1]) and e['n4'] > 100]\n",
    "print(len(l4x))\n",
    "l4x.sort(key=key_x4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2941be-db0f-449b-92c7-8b20a7dd36b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    print(l4x[i]['l4'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5d7ed5-2fda-4a05-bf65-4ac2ff696379",
   "metadata": {},
   "source": [
    "### 3. Get style vector of each\n",
    "\n",
    "#### 3.1 Load Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2671243a-d67f-43c5-a04c-da55c609191d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "adc45412-9534-4ee3-9dd9-71e3f19b3e00",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 3: Load Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8b9889-3e6b-4c21-8159-d40ff4dd94e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = EXPERIMENT_ARGS['model_path']\n",
    "ckpt = torch.load(model_path, map_location='cpu')\n",
    "\n",
    "opts = ckpt['opts']\n",
    "pprint.pprint(opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a42e41-c485-42fb-b3cd-7aafac1b5de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update the training options\n",
    "opts['checkpoint_path'] = model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce61f54b-efa0-4103-a01e-b9c4ad3c4abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "optss = Namespace(**opts)\n",
    "net = pSp(optss)\n",
    "net.load_weights()\n",
    "net.eval()\n",
    "print('Model successfully loaded!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05bcbb0-886d-4351-bf69-d5b1ed49c22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.latent_avg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55027cff-2963-4ac6-9a11-6c196c9f8348",
   "metadata": {},
   "outputs": [],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01fa84d-ca99-4118-9554-4928d3c849b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "im = Image.open('/home/extra/micheal/pixel2style2pixel/data/ioct/bscans/test/0c3839cd-0aa9-4e6e-bd4e-eb8f0520e2056578-012.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45294e3d-d63d-421e-9004-d27e48eae59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(im.convert('L')).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0111e53-3d8f-4a3e-8aa9-9e4b6d9e484f",
   "metadata": {},
   "source": [
    "## Step 4: Visualize Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b85b7cb-31ba-48ce-be08-728867ce3756",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = EXPERIMENT_ARGS[\"image_path\"]\n",
    "original_image = Image.open(image_path)\n",
    "# if opts.label_nc == 0:\n",
    "#     original_image = original_image.convert(\"RGB\")\n",
    "# else:\n",
    "#     original_image = original_image.convert(\"L\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe99675-e303-45ef-9804-ccb95a4f64c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(original_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d7d219-74a9-446e-92cd-4c8a2ab59c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "coach.train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d96ec4-41b0-4398-b8ba-27a8dcc37768",
   "metadata": {},
   "source": [
    "## Step 5: Feed to Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3472559-de79-4730-afcb-7a237d6dbcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_transforms = EXPERIMENT_ARGS['transform']\n",
    "transformed_image = img_transforms(original_image)\n",
    "print(\"Transformed segmentation is of shape:\", transformed_image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e26d8fa-5e62-4c0c-9199-ccf55f741634",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_on_batch(inputs, net, latent_mask=None):\n",
    "    if latent_mask is None:\n",
    "        result_batch, latents = net(inputs.float(), randomize_noise=False, return_latents=True)\n",
    "    else:\n",
    "        result_batch = []\n",
    "        latents = []\n",
    "        for image_idx, input_image in enumerate(inputs):\n",
    "            # get latent vector to inject into our input image\n",
    "            vec_to_inject = np.random.randn(1, 512).astype('float32')\n",
    "            _, latent_to_inject = net(torch.from_numpy(vec_to_inject).to(\"cuda\"),\n",
    "                                      input_code=True,\n",
    "                                      return_latents=True)\n",
    "            # get output image with injected style vector\n",
    "            res, latent = net(input_image.unsqueeze(0).to(\"cuda\").float(),\n",
    "                      latent_mask=latent_mask,\n",
    "                      inject_latent=latent_to_inject,\n",
    "                      return_latents=True)\n",
    "            result_batch.append(res)\n",
    "            latents.append(latent)\n",
    "        result_batch = torch.cat(result_batch, dim=0)\n",
    "        latents = torch.cat(latents, dim=0)\n",
    "    return result_batch, latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a0bc0d-16dd-45ab-b962-a1e902f10118",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:1')\n",
    "net = net_test\n",
    "net.load_weights()\n",
    "net.latent_avg = net.latent_avg.to(device)\n",
    "net = net.to(device)\n",
    "transformed_image = transformed_image.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75808c1-563a-495b-ac63-42d0e2ddd3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    tic = time.time()\n",
    "    result_image, latents = run_on_batch(transformed_image.unsqueeze(0), net, latent_mask=None)\n",
    "    toc = time.time()\n",
    "    print('Inference took {:.4f} seconds.'.format(toc - tic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508aea75-17c2-4453-ab23-9d680d331846",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"images shape\", result_image.shape)\n",
    "print(\"latents shape\", latents.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b45088-a52f-43a3-9727-93de1d8e286f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor2im(var, grayscale=False):\n",
    "    var = var.cpu().detach().transpose(0, 2).transpose(0, 1).numpy()\n",
    "    var = ((var + 1) / 2)\n",
    "    var[var < 0] = 0\n",
    "    var[var > 1] = 1\n",
    "    var = var * 255\n",
    "    if grayscale:\n",
    "        im = Image.fromarray(var.astype('uint8').squeeze(axis=2), 'L')\n",
    "    else:\n",
    "        im = Image.fromarray(var.astype('uint8'))\n",
    "    return im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f011cd61-bc4e-4879-b3f5-0d00d08ea791",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_image = tensor2im(result_image[0], grayscale=True)\n",
    "output_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec2f940-b7e0-43c9-b2eb-c29a7034320b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(output_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d008c128-2ae2-4a00-8f23-ae0672c217f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
